---
title: "Anomaly Detection Methodology"
author: "Nathan Hoteling"
date: "4/16/2021"
output: html_document
---

<hr>
<br>
<br>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)          # for pivot_longer()
library(readr)          # for parse_number()
library(stringr)
library(readtext)
library(fitdistrplus)   # fitting distributions
library(actuar)         # for pareto distribution
library(e1071)          # for Hamming distance
library(lubridate)
library(ggplot2)

library(solitude)  # for isolation forest stuff
```

## DATA

To illustrate the algorithm(s), we generate two simulated datasets consisting of ten variables measured over a period of 365 days.  Notice the pattern in the figures below.  Four simulated "anomalies" are injected into variable X04 at regular intervals.  The left plot shows a dense dataset and the right shows a sparse dataset.

```{r, include=FALSE}
#
# Make data
#
first_date <- ymd("2020-01-01")
n <- 365
dates=seq.Date(from = first_date, to = first_date+n-1, by="day")
vec <- c(1,1,3,3,5,9,11,9,5,3)
vec_sparse <- c(1,1,1,1,1,1,1,1,1)
fake_anomalies <- data.frame(date=ymd(c("2020-02-01", "2020-05-01", "2020-08-01", "2020-11-01")),
                             mag = c(5, 5, 5, 5))

#
# Create some random data: dense data
#
set.seed(1234)  # set seed so it's repeatable
d <- lapply(seq_len(length(vec)), function(i) {
  nrm <- rnorm(n, vec[i], 2.5)
  nrm[ nrm < 0 ] <- 0
  #nrm <- nrm / max(nrm)
  return(nrm)
})
df.dat1 <- data.frame(do.call(cbind,d)) %>%
  mutate(date=dates, id = "Data 1") %>%
  left_join(fake_anomalies, by="date") %>%
  mutate(X4=ifelse(!is.na(mag),vec[4]+mag*vec[4],X4)) %>%#,
         #X3=ifelse(!is.na(mag),X3+mag*X3,X3),
         #X4=ifelse(!is.na(mag),X4+mag*X4,X4)) %>%
  dplyr::select(-mag)



#
# Create some random data: sparse data
#
set.seed(1234)  # set seed so it's repeatable
d <- lapply(seq_len(length(vec_sparse)), function(i) {
  bin <- rbinom(n, vec_sparse[i], 0.000)
  #bin[ bin < 0 ] <- 0
  #nrm <- nrm / max(nrm)
  return(bin)
})
df.dat2 <- data.frame(X0=floor(rnorm(n, 3)), do.call(cbind,d)) %>%
  mutate(date=dates, id = "Data 1",
         X0 = ifelse(X0>0,X0,0)) %>%
  left_join(fake_anomalies, by="date") %>%
  mutate(X4=ifelse(!is.na(mag),rnorm(1,4),X4)) %>%#,
  dplyr::select(-mag) #%>%
  #rowwise() %>%
  #mutate(sm = sum(c_across(X0:X9))) %>%               # sum across columns
  #mutate(across(X0:X9, ~. / ifelse(sm>0,sm,1))) %>%   # normalize to sum
  #dplyr::select(-sm)                                  # drop sm variable





#
# Plot some data
#
p.tile1 <- df.dat1 %>%
  pivot_longer(cols = 1:10) %>%
  mutate(name = paste("X",str_pad(parse_number(name),2,pad="0"),sep="")) %>%
  ggplot() +
  geom_tile(aes(x=date, y=name, fill=value)) +
  scale_fill_gradient(low="grey90", high="grey20") +
  labs(title="Dense Data", y="Variables", x="") +
  theme_minimal() +
  theme(legend.position="none") 

p.tile2 <- df.dat2 %>%
  pivot_longer(cols = 1:10) %>%
  mutate(name = paste("X",str_pad(parse_number(name),2,pad="0"),sep="")) %>%
  ggplot() +
  geom_tile(aes(x=date, y=name, fill=value)) +
  scale_fill_gradient(low="grey90", high="grey20") +
  labs(title="Sparse Data", y="Variables", x="") +
  theme_minimal() +
  theme(legend.position="none")
```

```{r, echo=FALSE, out.width="50%"}
p.tile1
p.tile2
```

<br>

## QMETRIC


```{r, include=FALSE}

# Functions!
metric_euclidean <- function(df) {
  v_mean <- as.vector(t(colMeans(df)))
  d <- lapply(seq_len(nrow(df)), function(i) { 
    v1 <- as.vector(t(df[i,]))
    v2 <- v_mean
    dst <- as.numeric(dist(rbind(v1,v2), method="euclidean"))
    return(dst) 
  })
  return(unlist(d))
}

metric_mahalanobis <- function(df) {
  dst <- mahalanobis(df, colMeans(df), cov(df))
  return(dst)
}

metric_distance <- function(df, method="mahalanobis") {
  dst <- switch(method,
                  "euclidean" = metric_euclidean(df),
                  "mahalanobis" = metric_mahalanobis(df))
  return(dst) 
}
```



#### Method

The basic principle behind the algorithm is that one can treat a collection of variables measured over time as a series of vectors.  These vectors can be compared with some nominal value representing "normal".  If nothing is defined _a priori_ then it is simple enough to use a statistical measure like mean or median.  

Each vector is compared to the nominal value with a vector distance metric.  In this case, Euclidean Distance is used, but other options are available, like Mahalanobis, Manhattan, Minkowski, and others.

Once the vector distances are computed, the values are fit to some well-known distribution function, and a threshold value is derived from a user-defined probability value, similar to the selection of a p-value for significance measurements.  For example, the distributions below are fit to a Gamma Distribution function.  The threshold values are determined from the fits and represent the value for which there is a 1 in 1000 probability of randomly selecting a distance greater than this, given the fitted distribution.  The fit looks good for the Dense dataset, but the fit is questionable for the sparse dataset.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
k <- 0.001  # threshold

# Part 1: Dense data
df1 <- df.dat1
dst1 <- metric_distance(df1[,-c(11,12)], method="euclidean")                    # vector distance
df1$dst <- dst1
ft1 <- suppressWarnings(fitdist(df1$dst, "gamma", method = "mle"))              # fit distances
gf1 <- gofstat(ft1)                                                             # fit stats
k1 <- qgamma(k, ft1$estimate["shape"], ft1$estimate["rate"], lower.tail=FALSE)  # get threshold

# Part 2: Sparse data
df2 <- df.dat2
dst2 <- metric_distance(df2[,-c(11,12)], method="euclidean")                    # vector distance
df2$dst <- dst2
ft2 <- suppressWarnings(fitdist(df2$dst, "gamma", method = "mle"))              # fit distances
gf2 <- gofstat(ft2)                                                             # fit stats
k2 <- qgamma(k, ft2$estimate["shape"], ft2$estimate["rate"], lower.tail=FALSE)  # get threshold




# Make some plots
n_bins <- 50
mx1 <- max(density(dst1, n=n_bins)$y)
p.den1 <- ggplot(df1) + 
  geom_histogram(aes(x=dst, y=..density..), bins=n_bins) +
  geom_function(fun=dgamma, args=list("shape"=ft1$estimate["shape"],
                                      "rate"=ft1$estimate["rate"]),
                color="red", size=0.6, alpha=0.8) +
  geom_segment(aes(x=k1, xend=k1,y=0,yend=0.5*mx1), color="red") +
  annotate("text", x=k1, y=0.5*mx1, label=paste("Threshold =",round(k1, digits=0)),
           hjust=-0.1, color="red") +
  annotate("text", x=0.8*max(df1$dst), y=Inf, 
           label=paste("Goodness of Fit:", "\n",
                       "KS = ",round(gf1$ks, digits=2), " (",gf1$kstest,")","\n",
                       "AD = ",round(gf1$ad, digits=2), " (",gf1$adtest,")","\n",
                       "CM = ",round(gf1$cvm, digits=2), " (",gf1$cvmtest,")",sep=""),
           hjust=0.0,vjust=1.5, color="grey50") +
  labs(x="Euclidean Distance", y="", title="Gamma Distribution | Dense Data") +
  theme_minimal()

mx2 <- max(density(dst2, n=n_bins)$y)
p.den2 <- ggplot(df2) + 
  geom_histogram(aes(x=dst, y=..density..), bins=n_bins) +
  geom_function(fun=dgamma, args=list("shape"=ft2$estimate["shape"],
                                      "rate"=ft2$estimate["rate"]),
                color="red", size=0.6, alpha=0.8) +
  geom_segment(aes(x=k2, xend=k2,y=0,yend=0.5*mx2), color="red") +
  annotate("text", x=k2, y=0.5*mx2, label=paste("Threshold =",round(k2, digits=0)),
           hjust=-0.1, color="red") +
  annotate("text", x=0.75*max(df2$dst), y=Inf, 
           label=paste("Goodness of Fit:", "\n",
                       "KS = ",round(gf2$ks, digits=2), " (",gf2$kstest,")","\n",
                       "AD = ",round(gf2$ad, digits=2), " (",gf2$adtest,")","\n",
                       "CM = ",round(gf2$cvm, digits=2), " (",gf2$cvmtest,")",sep=""),
           hjust=0.0, vjust=1.5, color="grey50") +
  labs(x="Euclidean Distance", y="", title="Gamma Distribution | Sparse Data") +
  theme_minimal()
  
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width="50%"}
p.den1
p.den2
```

<br>

#### Anomaly Metric

The anomaly metric is computed by normalizing the distances to the threshold value derived from the distribution fits.  In this way, anomalies can be quickly identified as any data point with a value greater than 1.0.  The figures below show the anomaly metric values determined from the simulated dataset.  The anomaly locations are depicted by grey bars and the algorithm results are highlighted by red dots.  Note that, although the fit was lousy for sparse data, the anomalies appear quite clear here; this won't always be the case.

```{r, include=FALSE}

# Calculate the anomaly metric
df1$qmetric <- dst1 / k1
df2$qmetric <- dst2 / k2


# Make some plots
p.m1 <- ggplot(df1) + 
  geom_segment(data=fake_anomalies, aes(x=date,xend=date,y=0,yend=Inf), size=3, color="grey80", alpha=0.5) +
  geom_col(aes(x=date, y=qmetric), width=1.0) + 
  geom_point(data=df1 %>% filter(qmetric>=1.0), mapping=aes(x=date, y=qmetric), size=2.5, color="red") +
  labs(x="", y="", title="Anomaly Metric | Dense Data") +
  theme_minimal()

p.m2 <- ggplot(df2) + 
  geom_segment(data=fake_anomalies, aes(x=date,xend=date,y=0,yend=Inf), size=3, color="grey80", alpha=0.5) +
  geom_col(aes(x=date, y=qmetric), width=1.0) + 
  geom_point(data=df2 %>% filter(qmetric>=1.0), mapping=aes(x=date, y=qmetric), size=2.5, color="red") +
  labs(x="",y="", title="Anomaly Metric | Sparse Data") +
  theme_minimal()
```


```{r, echo=FALSE, out.width="50%"}
p.m1
p.m2
```

<br>

## IFOREST


#### Method

The Isolation Forest method, or IFOREST, is implemented via `solitude`.  The method uses a procedure analogous to a binary tree to determine path-length from the root level to a particular node; the idea is that so-called "anomalous" data will exhibit shorter path-lengths since they are less similar to most of the other data and therefore are separated out earlier in the binary tree process.  In general, the algorithm authors suggest the following regarding determination of an anomaly threshold:
<ul>
  <li>If anomaly_score is close to 1 then they are definitely anomalies</li>
  <li>If anomaly_score is much smaller than 0.5 then they can safely be regarded as not anomalies</li>
  <li>If all instances return values close to 0.5 then there aren't any distinct anomalies</li>
</ul>

The authors further suggest using quantiles to determine a significance threshold, however the exact level is entirely up to the user.  In the example here, we use the 99th percentile to determine the significance threshold.

```{r, include=FALSE, message=FALSE, warning=FALSE}
df.ifr1 <- df.dat1
df.ifr2 <- df.dat2

# TODO: Put this all in a function

# initiate an isolation forest
iso1 = isolationForest$new(sample_size = nrow(df.ifr1))
iso1$fit(df.ifr1[,-c(11:12)])
iso2 = isolationForest$new(sample_size = nrow(df.ifr2))
iso2$fit(df.ifr2[,-c(11:12)])



# Obtain anomaly scores 1
df.scores1 = iso1$predict(df.ifr1)
lo1 <- min(df.scores1$anomaly_score)
df.scores1$iforest <- df.scores1$anomaly_score - lo1
thresh1 <- as.vector(quantile(df.scores1$anomaly_score, 0.99))
#thresh1 <- mean(df.scores$iforest) + 3*sd(df.scores$iforest)
df.ifr1$imetric <- df.scores1$iforest/(thresh1-lo1)

# Obtain anomaly scores 2
df.scores2 = iso2$predict(df.ifr2)
lo2 <- min(df.scores2$anomaly_score)
df.scores2$iforest <- df.scores2$anomaly_score - lo2
thresh2 <- as.vector(quantile(df.scores2$anomaly_score, 0.99))
#thresh2 <- mean(df.scores$iforest) + 3*sd(df.scores$iforest)
df.ifr2$imetric <- df.scores2$iforest/(thresh2-lo2)


mx1i <- max(density(df.scores1$anomaly_score, n=n_bins)$y)
p.if1 <- ggplot(df.scores1) +
  geom_histogram(aes(x=anomaly_score, y=..density..), bins=n_bins) +
  geom_segment(aes(x=thresh1, xend=thresh1,y=0,yend=0.5*mx1i), color="red") +
  annotate("text", x=thresh1, y=0.5*mx1i, label=paste("Threshold =",round(thresh1, digits=2)),
           hjust=-0.1, color="red") +
  labs(title="Isolation Forest | Dense Data",x="Anomaly Score",y="") +
  theme_minimal()

mx2i <- max(density(df.scores2$anomaly_score, n=n_bins)$y)
p.if2 <- ggplot(df.scores2) +
  geom_histogram(aes(x=anomaly_score, y=..density..),bins=n_bins) +
  geom_segment(aes(x=thresh2, xend=thresh2,y=0,yend=0.5*mx2i), color="red") +
  annotate("text", x=thresh2, y=0.5*mx2i, label=paste("Threshold =",round(thresh2, digits=2)),
           hjust=-0.1, color="red") +
  labs(title="Isolation Forest | Sparse Data",x="Anomaly Score",y="") +
  theme_minimal()
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, out.width="50%"}
p.if1
p.if2
```

#### Anomaly Metric

We convert the anomaly score depicted above into an anomaly metric in an analogous way that we converted the Euclidean Distance above.


```{r, include=FALSE}


p.ifr1 <- ggplot(df.ifr1) + 
  geom_segment(data=fake_anomalies, aes(x=date,xend=date,y=0,yend=Inf), 
               size=3, color="grey80", alpha=0.5) +
  geom_col(aes(x=date, y=imetric), width=1.0) + theme_minimal() +
  geom_point(data=df.ifr1 %>% filter(imetric > 1.0), mapping=aes(x=date, y=imetric), 
             size=2.5, color="red") +
  labs(x="", y="", title="Isolation Forest | Dense Data") +
  theme_minimal()


p.ifr2 <- ggplot(df.ifr2) + 
  geom_segment(data=fake_anomalies, aes(x=date,xend=date,y=0,yend=Inf), 
               size=3, color="grey80", alpha=0.5) +
  geom_col(aes(x=date, y=imetric), width=1.0) + theme_minimal() +
  geom_point(data=df.ifr2 %>% filter(imetric > 1.0), mapping=aes(x=date, y=imetric), 
             size=2.5, color="red") +
  labs(x="", y="", title="Isolation Forest | Sparse Data") +
  theme_minimal()

#p.tst
#p.tst2 <- ggplot(df.dat2) + geom_col(aes(x=date, y=X0), width=1.0) + theme_minimal()
```

```{r, echo=FALSE, out.width="50%"}
p.ifr1
p.ifr2
```





## References

Nice description of Mahalanobis Distance and other stats things:  
https://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html  
https://blogs.sas.com/content/iml/2012/02/02/detecting-outliers-in-sas-part-3-multivariate-location-and-scatter.html  


Nice description of Gamma Distribution:  
https://towardsdatascience.com/gamma-distribution-intuition-derivation-and-examples-55f407423840  

Isolation Forest:  
https://talegari.github.io/solitude/  
https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf  

See here for an R-focused discussion on Survival Analysis:  
https://www.r-bloggers.com/2020/01/survival-analysis-fitting-weibull-models-for-improving-device-reliability-in-r/

